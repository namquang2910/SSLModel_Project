{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bdf74aa-e5ad-4225-b4b0-a733e28d17f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import wfdb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import resample, medfilt\n",
    "import pywt\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import csv \n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import neurokit2 as nk\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4352ea-9362-4248-8724-b7ed365272ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(data):\n",
    "    coeffs = pywt.wavedec(data, 'db4', level=5) #Wavelet decomposition, reduces noise, enhances key elements\n",
    "    data = pywt.waverec(coeffs, 'db4')\n",
    "    data = medfilt(data, kernel_size=5) #Reduces noise further\n",
    "    data = nk.signal_resample(signal=data, sampling_rate=360, desired_sampling_rate=100)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1585c7-17c3-4186-b45a-6647d57ff676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_record(record_id, seq_len=1000, stride=1000):\n",
    "    record_name = str(record_id)\n",
    "    if not os.path.exists(f\"mit_bih/{record_name}.dat\"):\n",
    "        wfdb.dl_database('mitdb', './', records=[record_name])\n",
    "    record = wfdb.rdrecord('mit_bih/' + record_name)\n",
    "    \n",
    "    #Extract each channel\n",
    "    signal_data = record.p_signal\n",
    "    ch1 = preprocessing_data(signal_data[:, 0]).reshape(-1, 1)\n",
    "    ch2 = preprocessing_data(signal_data[:, 1]).reshape(-1, 1)\n",
    "\n",
    "    #Scale the signal\n",
    "    scaler1 = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "    scaler2 = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "    ch1_scale = scaler1.fit_transform(ch1)\n",
    "    ch2_scale = scaler2.fit_transform(ch2)\n",
    "\n",
    "    \n",
    "    # Create sequences using sliding window\n",
    "    seq_ch1 = [ch1_scale[i:i+seq_len] for i in range(0, len(ch1_scale)-seq_len+1, stride)]\n",
    "    seq_ch2 = [ch2_scale[i:i+seq_len] for i in range(0, len(ch2_scale)-seq_len+1, stride)]\n",
    "    seq_ch1.extend(seq_ch2)\n",
    "    return seq_ch1\n",
    "\n",
    "def load_multiple_records(record_ranges, seq_len=1000, stride_len=100):\n",
    "    all_data = []\n",
    "\n",
    "    for start, end in tqdm(record_ranges):\n",
    "        for record_id in range(start, end + 1):\n",
    "            record_data = load_record(record_id, seq_len, stride_len)\n",
    "            all_data.extend(record_data)\n",
    "    # Save to pkl\n",
    "    with open('database.pkl', 'wb') as f:\n",
    "        pickle.dump(all_data, f)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a079e310-967f-470b-972e-1be422dedf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data, original_rate, target_rate):\n",
    "    num_samples = int(len(data) * target_rate / original_rate)\n",
    "    return resample(data, num_samples)\n",
    "\n",
    "def calculate_l1(predictions, targets):\n",
    "    l1_norm = torch.norm(predictions-targets, p=1)\n",
    "    l1_sum = torch.sum(torch.abs(targets))\n",
    "    accuracy = 100 * (1 - (l1_norm / l1_sum))\n",
    "    return accuracy\n",
    "\n",
    "def calculate_l2(predictions, targets):\n",
    "    l2_norm = torch.norm(predictions - targets, p=2)\n",
    "    l2_sum = torch.norm(targets, p=2)\n",
    "    accuracy = 100 * (1 - (l2_norm / l2_sum))\n",
    "    return accuracy\n",
    "\n",
    "def calculate_MAPE(predictions, targets):\n",
    "    mape = torch.mean(torch.abs((targets - predictions) / (targets)))\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b8e547-99bd-4efc-abde-f01d2482b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_position_mask_peaks(rpeaks, seq, mask_length, num_peaks_to_mask):\n",
    "    mask = np.zeros_like(seq, dtype=bool)  # Create a mask with the same length as seq, initially all False\n",
    "\n",
    "    if len(rpeaks) > 0:  # If there are detected peaks\n",
    "        # Randomly select 'num_peaks_to_mask' peaks from rpeaks\n",
    "        selected_peaks = rpeaks[1:2*num_peaks_to_mask + 1: 2]\n",
    "\n",
    "        for peak in selected_peaks:\n",
    "            mask_half = mask_length // 2\n",
    "            start = int(max(0, peak - mask_half))\n",
    "            end = int(min(len(seq), peak + mask_half))\n",
    "            mask[start:end] = True\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def fixed_position_mask_P_T(rpeaks, seq, mask_length, num_peaks_to_mask):\n",
    "    mask = np.zeros_like(seq, dtype=bool)  # Create a mask with the same length as seq, initially all False\n",
    "\n",
    "    if len(rpeaks) > 0:  # If there are detected peaks\n",
    "        # Randomly select 'num_peaks_to_mask' peaks from rpeaks\n",
    "        selected_peaks = rpeaks[1:2*num_peaks_to_mask + 1: 2]\n",
    "\n",
    "        for peak in selected_peaks:\n",
    "            P_start = int(max(0, peak - 15 - mask_length))\n",
    "            P_end = int(max(0, peak + 15))\n",
    "            T_start = int(max(0, peak + 15))\n",
    "            T_end = int(min(0, peak + 15 + mask_length))            \n",
    "            mask[P_start:P_end] = True\n",
    "            mask[T_start:T_end] = True\n",
    "    return mask\n",
    "\n",
    "def fixed_position_mask_peaks_int(rpeaks, seq, mask_length, num_peaks_to_mask):\n",
    "    mask = np.ones_like(seq, dtype=float)  # Create a mask with the same length as seq, initially all False\n",
    "\n",
    "    if len(rpeaks) > 0:  # If there are detected peaks\n",
    "        # Randomly select 'num_peaks_to_mask' peaks from rpeaks\n",
    "        selected_peaks = rpeaks[1:2*num_peaks_to_mask + 1: 2]\n",
    "\n",
    "        for peak in selected_peaks:\n",
    "            mask_half = mask_length // 2\n",
    "            start = int(max(0, peak - mask_half))\n",
    "            end = int(min(len(seq), peak + mask_half))\n",
    "            mask[start:end] = 0\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4e671d-c62c-466a-b322-129ab39daf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=1000, mask_length=30, target_rate=100, num_peaks_to_mask=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.array): Full ECG signal data.\n",
    "            seq_len (int): Length of each sequence before downsampling.\n",
    "            mask_length (int): Length of masking window around the first R-peak.\n",
    "            target_rate (int): Target sampling rate after downsampling.\n",
    "            augment (bool): Whether to apply data augmentation.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.mask_length = mask_length\n",
    "        self.target_rate = target_rate\n",
    "        self.num_peaks_to_mask = num_peaks_to_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx].flatten()\n",
    "        # Detect R-peaks\n",
    "        rpeaks = nk.ecg_findpeaks(seq, sampling_rate=self.target_rate)['ECG_R_Peaks']\n",
    "\n",
    "        # Create masking\n",
    "        mask_r_peak = fixed_position_mask_peaks(rpeaks, seq, self.mask_length, self.num_peaks_to_mask)\n",
    "        mask_p_t_peak = fixed_position_mask_P_T(rpeaks, seq, 15, self.num_peaks_to_mask)\n",
    "\n",
    "      #  masked_seq = seq.copy()\n",
    "        # masked_seq[mask] = 0  # Apply masking\n",
    "\n",
    "        return (\n",
    "            torch.tensor(seq, dtype=torch.float32),\n",
    "            torch.tensor(mask_r_peak, dtype=torch.bool),\n",
    "            torch.tensor(mask_p_t_peak, dtype=torch.bool)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9e34a87-d63c-4254-986c-dbc1d0ed9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskingLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, seq, mask):\n",
    "        seq = seq.clone()   # avoid in-place ops on computation graph\n",
    "        seq[mask] = 0\n",
    "        return seq\n",
    "\n",
    "\n",
    "class Encoder1D_Mask(nn.Module):\n",
    "    class Conv_Block(nn.Module):\n",
    "        def __init__(self, embed_dim=32, hidden_dim=64, lstm_hidden_dim=128):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv1d(1, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.conv2 = nn.Conv1d(embed_dim, embed_dim * 2, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.conv3 = nn.Conv1d(embed_dim * 2, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "            self.bilstm = nn.LSTM(hidden_dim, lstm_hidden_dim, bidirectional=True, batch_first=True)\n",
    "        def forward(self, x):\n",
    "            # Input: (B, L) → (B, 1, L)\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            x = F.leaky_relu(self.conv1(x), negative_slope=0.01)\n",
    "            x = self.pool1(x)\n",
    "            x = F.leaky_relu(self.conv2(x), negative_slope=0.01)\n",
    "            x = self.pool2(x)\n",
    "            x = F.leaky_relu(self.conv3(x), negative_slope=0.01)\n",
    "            \n",
    "            x = x.permute(0, 2, 1)\n",
    "            x, _ = self.bilstm(x)\n",
    "            x = x.permute(0, 2, 1)  # (B, 2*lstm_hidden_dim, L/4)\n",
    "            return x\n",
    "\n",
    "    def __init__(self, embed_dim=32, hidden_dim=64, lstm_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.mask_peaks = MaskingLayer()\n",
    "        self.QRS_Conv_Block = self.Conv_Block(embed_dim, hidden_dim)\n",
    "        self.P_T_Conv_Block = self.Conv_Block(embed_dim, hidden_dim)\n",
    "        self.fusion =  nn.Sequential(\n",
    "            nn.Linear(2*lstm_hidden_dim*2, lstm_hidden_dim*2),  # 2 branches * bidirectional LSTM\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(lstm_hidden_dim*2, lstm_hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask_qrs, mask_pt):\n",
    "        # Apply masking\n",
    "        qrs_x = self.mask_peaks(x, mask_qrs)\n",
    "        pt_x = self.mask_peaks(x, mask_pt)\n",
    "\n",
    "        # Encode each branch\n",
    "        qrs_x = self.QRS_Conv_Block(qrs_x)\n",
    "        pt_x = self.P_T_Conv_Block(pt_x)\n",
    "\n",
    "        # Concatenate features along channels\n",
    "        x = torch.cat([qrs_x, pt_x], dim=1)  # (B, hidden_dim*2, L/4)\n",
    "\n",
    "        x = self.fusion(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder1D(nn.Module):\n",
    "    def __init__(self, embed_dim=32, hidden_dim=64, lstm_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.Conv1d(lstm_hidden_dim * 2, embed_dim * 2,\n",
    "                                 kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.Conv1d(embed_dim * 2, embed_dim,\n",
    "                                 kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv3 = nn.Conv1d(embed_dim, 1,\n",
    "                                 kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.deconv1(x), negative_slope=0.01)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='linear', align_corners=True)\n",
    "\n",
    "        x = F.leaky_relu(self.deconv2(x), negative_slope=0.01)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='linear', align_corners=True)\n",
    "\n",
    "        x = self.deconv3(x)  # (B, 1, L)\n",
    "        return x  # keep (B, 1, L)\n",
    "\n",
    "\n",
    "class MAE1D_Mask(nn.Module):\n",
    "    def __init__(self, embed_dim=32, hidden_dim=64, lstm_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder1D_Mask(embed_dim, hidden_dim, lstm_hidden_dim)\n",
    "        self.decoder = Decoder1D(embed_dim, hidden_dim, lstm_hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask_qrs, mask_pt):\n",
    "        x = self.encoder(x, mask_qrs, mask_pt)\n",
    "        x = self.decoder(x)\n",
    "        return x.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abd88e0e-a8b5-49c3-842d-e46072079ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "def prepare_data(data, seq_len, num_rpeaks, mask_len, batch_size=128):\n",
    "    dataset = ECGDataset(data, seq_len, mask_length = mask_len, num_peaks_to_mask = num_rpeaks)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef5b6146-fa0f-4492-8c80-7f41dca938d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, seq_len=5000, num_rpeak=1, log_dir=None, masking_length = 30,scale = 'minmax_0_5',test_case = True):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_rpeak = num_rpeak\n",
    "        self.mask_len = masking_length\n",
    "        self.scale = scale\n",
    "        self.device = ( \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.log_dir = log_dir\n",
    "        self.test_case = test_case\n",
    "        self.cur_time = datetime.now(ZoneInfo(\"Australia/Sydney\")).strftime(\"%H%M_%d%m%Y\")\n",
    "        if self.log_dir is None:\n",
    "            self.log_dir = f'runs/seq{seq_len}_rpeak{num_rpeak}_{self.cur_time}'\n",
    "        if self.test_case == False:\n",
    "            self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "\n",
    "    def run(self, train_loader, val_loader, test_loader, epochs=100, patience = 10):\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for original_seq, mask_qrs, mask_pt in train_loader:\n",
    "                original_seq, mask_qrs,mask_pt = original_seq.to(self.device), mask_qrs.to(self.device), mask_pt.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                reconstructed = self.model(original_seq, mask_qrs, mask_pt)\n",
    "\n",
    "                loss_qrs = self.criterion(reconstructed[mask_qrs], original_seq[mask_qrs])\n",
    "                loss_pt = self.criterion(reconstructed[mask_pt], original_seq[mask_pt])\n",
    "                loss_masked = 0.7 * loss_qrs + 0.3 * loss_pt\n",
    "                # full reconstruction loss to preserve unmasked regions (small weight)\n",
    "                loss_full = self.criterion(reconstructed, original_seq)\n",
    "                \n",
    "                # combine: favor masked loss but still penalize drifting on unmasked\n",
    "                alpha = 1.0   # weight for masked-target objective\n",
    "                beta  = 0.1  # small weight for full-signal reconstruction\n",
    "                loss = alpha * loss_masked + beta * loss_full\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "\n",
    "            avg_val_loss, avg_l1, avg_l2, avg_MAPE = self.evaluate(epoch=epoch, data_loader = val_loader)\n",
    "            self.val_losses.append(avg_val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "                  f\"L1 Accuracy: {avg_l1:.2f}, L2 Accuracy: {avg_l2:.2f}, MAPE Accuracy: {avg_MAPE:.2f}\")\n",
    "            if self.test_case == False:\n",
    "                self.writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "                self.writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "                self.writer.add_scalar(\"Accuracy/L1\", avg_l1, epoch)\n",
    "                self.writer.add_scalar(\"Accuracy/L2\", avg_l2, epoch)\n",
    "                self.writer.add_scalar(\"Accuracy/MAPE\", avg_MAPE, epoch)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{self.log_dir}/model_{self.seq_len}_{self.num_rpeak}_{self.cur_time}.pth\")\n",
    "                if self.test_case == False:\n",
    "                    print(\"Saving the best model checkpoint...\")\n",
    "                    torch.save(self.model.state_dict(), f\"{self.log_dir}/model_{self.seq_len}_{self.num_rpeak}_{self.cur_time}.pth\")\n",
    "\n",
    "        self.writer.close()\n",
    "        if test_loader is None:\n",
    "            self.final_evaluation(val_loader)\n",
    "        else: \n",
    "            self.final_evaluation(test_loader)\n",
    "            \n",
    "    def evaluate(self, data_loader, epoch= None):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        l1_accuracy = 0.0\n",
    "        l2_accuracy = 0.0\n",
    "        MAPE_accuracy = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for original_seq, mask_qrs, mask_pt in data_loader:\n",
    "                original_seq, mask_qrs,mask_pt = original_seq.to(self.device), mask_qrs.to(self.device), mask_pt.to(self.device)\n",
    "                reconstructed = self.model(original_seq, mask_qrs, mask_pt)\n",
    "                \n",
    "                loss_qrs = self.criterion(reconstructed[mask_qrs], original_seq[mask_qrs])\n",
    "                loss_pt = self.criterion(reconstructed[mask_pt], original_seq[mask_pt])\n",
    "                loss_masked = 0.7 * loss_qrs + 0.3 * loss_pt\n",
    "                # full reconstruction loss to preserve unmasked regions (small weight)\n",
    "                loss_full = self.criterion(reconstructed, original_seq)\n",
    "                \n",
    "                # combine: favor masked loss but still penalize drifting on unmasked\n",
    "                alpha = 1.0   # weight for masked-target objective\n",
    "                beta  = 0.1  # small weight for full-signal reconstruction\n",
    "                loss = alpha * loss_masked + beta * loss_full\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                l1_accuracy += calculate_l1(reconstructed[mask_qrs+mask_pt], original_seq[mask_qrs+mask_pt])\n",
    "                l2_accuracy += calculate_l2(reconstructed[mask_qrs+mask_pt], original_seq[mask_qrs+mask_pt])\n",
    "                MAPE_accuracy += calculate_MAPE(reconstructed[mask_qrs+mask_pt], original_seq[mask_qrs+mask_pt])\n",
    "\n",
    "        avg_val_loss = val_loss / len(data_loader)\n",
    "        avg_l1 = l1_accuracy / len(data_loader)\n",
    "        avg_l2 = l2_accuracy / len(data_loader)\n",
    "        avg_MAPE = MAPE_accuracy / len(data_loader)\n",
    "\n",
    "        if epoch is None:\n",
    "            print(f\"\\nValidation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"L1 Accuracy: {avg_l1:.4f}\")\n",
    "            print(f\"L2 Accuracy: {avg_l2:.4f}\")\n",
    "            print(f\"MAPE Accuracy: {avg_MAPE:.2f}%\")\n",
    "        return avg_val_loss, avg_l1, avg_l2, avg_MAPE\n",
    "        \n",
    "    def plot_predictions_with_residuals(self, data_loader, sample_len = None, num_samples=2 ):\n",
    "        self.model.eval()\n",
    "        all_actual, all_predicted, all_masks = [], [], []\n",
    "        if sample_len is None:\n",
    "            sample_len = self.seq_len\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for original_seq, mask_qrs, mask_pt in data_loader:\n",
    "                original_seq, mask_qrs,mask_pt = original_seq.to(self.device), mask_qrs.to(self.device), mask_pt.to(self.device)\n",
    "                reconstructed = self.model(original_seq, mask_qrs, mask_pt)\n",
    "                mask = mask_qrs + mask_pt\n",
    "                all_actual.extend(original_seq.cpu().numpy())  # original (unmasked) ECG\n",
    "                all_predicted.extend(reconstructed.cpu().numpy())  # predictions\n",
    "                all_masks.extend(mask.cpu().numpy()) # Masked regions\n",
    "     \n",
    "                break  # Take only one batch for plotting\n",
    "        \n",
    "        # Flatten the lists for metric calculation\n",
    "        all_actual_flat = np.concatenate(all_actual)\n",
    "        all_predicted_flat = np.concatenate(all_predicted)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(all_actual_flat, all_predicted_flat)\n",
    "        mae = mean_absolute_error(all_actual_flat, all_predicted_flat)\n",
    "        r2 = r2_score(all_actual_flat, all_predicted_flat)\n",
    "    \n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"R-squared (R²): {r2:.4f}\")\n",
    "        \n",
    "        # Visualize predictions vs. actual data with residuals\n",
    "        plt.figure(figsize=(15, num_samples * 5))\n",
    "        for i in range(min(num_samples, len(all_actual))):\n",
    "            original = all_actual[i]\n",
    "            predicted = all_predicted[i]\n",
    "            mask = all_masks[i]\n",
    "    \n",
    "            # Plot predictions vs actual data\n",
    "            plt.subplot(num_samples, 2, i * 2 + 1)\n",
    "            plt.plot(original[:sample_len], label=\"Original\", color=\"blue\", alpha=0.7)\n",
    "            plt.plot(predicted[:sample_len], label=\"Predicted\", color=\"orange\", alpha=0.7)\n",
    "    \n",
    "            # Highlight masked regions\n",
    "            masked_indices = np.where(mask)[0]\n",
    "            plt.scatter(masked_indices, original[masked_indices], color=\"red\", label=\"Masked Regions\", alpha=0.7)\n",
    "    \n",
    "            plt.title(f\"Sample {i + 1} - Predictions\")\n",
    "            plt.legend()\n",
    "    \n",
    "            # Compute and plot residuals\n",
    "            residuals = np.array(original) - np.array(predicted)\n",
    "            plt.subplot(num_samples, 2, i * 2 + 2)\n",
    "            plt.plot(residuals[:sample_len], label=\"Residuals\", color=\"green\")\n",
    "            plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "            plt.title(f\"Sample {i + 1} - Residuals\")\n",
    "            plt.legend()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_curves(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label=\"Train Loss\", marker='o')\n",
    "        plt.plot(self.val_losses, label=\"Validation Loss\", marker='o')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Curves\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    def final_evaluation(self, data_loader):\n",
    "        avg_val_loss, avg_l1, avg_l2, avg_MAPE = self.evaluate(data_loader, epoch= None)\n",
    "        with open('runs/Model_Running.csv', 'a') as f_object:\n",
    "            writer_object = csv.writer(f_object)\n",
    "            writer_object.writerow([f'model_{self.seq_len}_{self.num_rpeak}_{self.cur_time}.pth', avg_l1.cpu().numpy(), avg_l2.cpu().numpy(), avg_MAPE.cpu().numpy(), self.seq_len, self.num_rpeak, self.mask_len, self.scale])\n",
    "            f_object.close()  \n",
    "        self.plot_predictions_with_residuals(data_loader, sample_len = 1000)\n",
    "        self.plot_loss_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab7c8c12-3e4a-44d0-ba21-88218786c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_sample(data_loader, device, num_samples=5):\n",
    "    all_actual, all_masks = [], []\n",
    "    with torch.no_grad():\n",
    "        for masked_seq, original_seq, mask in data_loader:\n",
    "            all_actual.extend(original_seq)  # original (unmasked) ECG\n",
    "            all_masks.extend(mask) # Masked regions\n",
    "            break\n",
    "            \n",
    "    plt.figure(figsize=(15, num_samples * 5))\n",
    "    for i in range(min(num_samples, len(all_actual))):\n",
    "        original = all_actual[i]\n",
    "        mask = all_masks[i]\n",
    "\n",
    "        # Plot predictions vs actual data\n",
    "        plt.subplot(num_samples, 2, i * 2 + 1)\n",
    "        plt.plot(original, label=\"Original\", color=\"blue\", alpha=0.7)\n",
    "\n",
    "        # Highlight masked regions\n",
    "        masked_indices = np.where(mask)[0]\n",
    "        plt.scatter(masked_indices, original[masked_indices], color=\"red\", label=\"Masked Regions\", alpha=0.7)\n",
    "\n",
    "        plt.title(f\"Sample {i + 1} - Predictions\")\n",
    "        plt.legend()    \n",
    "\n",
    "def plot_ecg_sample_no_mask(data, num_samples=2):\n",
    "    # Visualize predictions vs. actual data with residuals\n",
    "    fig, ax = plt.subplots(1, num_samples, figsize = (16,9))\n",
    "    ax = ax.flatten()\n",
    "    for i in range(0, num_samples):\n",
    "        # Plot predictions vs actual data\n",
    "        ax[i].plot(data[i], label=\"Original\", color=\"blue\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9d5d2b0-2458-4747-ac0e-71a9925b4686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  2.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Training the model with 6 R peaks\n",
      "====================\n",
      "Preparing the data\n",
      "Training the model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (65536x250 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(mae_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)    \n\u001b[1;32m     24\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m Trainer(model \u001b[38;5;241m=\u001b[39m mae_model, \n\u001b[1;32m     25\u001b[0m                         criterion \u001b[38;5;241m=\u001b[39m criterion, optimizer \u001b[38;5;241m=\u001b[39m optimizer,  \n\u001b[1;32m     26\u001b[0m                         seq_len\u001b[38;5;241m=\u001b[39mseq_len, masking_length \u001b[38;5;241m=\u001b[39m mask_length, num_rpeak\u001b[38;5;241m=\u001b[39mnum, \n\u001b[1;32m     27\u001b[0m                         scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinmax_05\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m                         test_case \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, train_loader, val_loader, test_loader, epochs, patience)\u001b[0m\n\u001b[1;32m     29\u001b[0m original_seq, mask_qrs,mask_pt \u001b[38;5;241m=\u001b[39m original_seq\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), mask_qrs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), mask_pt\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_qrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_pt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss_qrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(reconstructed[mask_qrs], original_seq[mask_qrs])\n\u001b[1;32m     35\u001b[0m loss_pt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(reconstructed[mask_pt], original_seq[mask_pt])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[48], line 96\u001b[0m, in \u001b[0;36mMAE1D_Mask.forward\u001b[0;34m(self, x, mask_qrs, mask_pt)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask_qrs, mask_pt):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_qrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_pt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[48], line 64\u001b[0m, in \u001b[0;36mEncoder1D_Mask.forward\u001b[0;34m(self, x, mask_qrs, mask_pt)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Concatenate features along channels\u001b[39;00m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([qrs_x, pt_x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, hidden_dim*2, L/4)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (65536x250 and 512x256)"
     ]
    }
   ],
   "source": [
    "record_ranges = [(100,109),(111, 119),(121,124),(200,203),(205,205),(207,210),(212,215),(217,217),(219,223),(228,228),(230,232)]\n",
    "print(\"Working with data\")\n",
    "seq_len = 1000\n",
    "stride = 1000\n",
    "mask_length = 30\n",
    "data = np.array(load_multiple_records(record_ranges, seq_len, stride))\n",
    "test_data = load_multiple_records([(233,234)], seq_len, stride)\n",
    "\n",
    "for num in [6]: \n",
    "    print(\"=\"*20)\n",
    "    print(f\"Training the model with {num} R peaks\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "    print(\"Preparing the data\")\n",
    "    train_loader, val_loader = prepare_data(data,num_rpeaks = num, seq_len=seq_len, mask_len = mask_length)\n",
    "    test_dataset = ECGDataset(test_data, seq_len, num_peaks_to_mask = num)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    print(\"Training the model\")\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_model = MAE1D_Mask()\n",
    "    optimizer = optim.Adam(mae_model.parameters(), lr=0.001)    \n",
    "\n",
    "    model_trainer = Trainer(model = mae_model, \n",
    "                            criterion = criterion, optimizer = optimizer,  \n",
    "                            seq_len=seq_len, masking_length = mask_length, num_rpeak=num, \n",
    "                            scale = 'Minmax_05',\n",
    "                            test_case = False)\n",
    "    model_trainer.run(train_loader, val_loader, test_loader, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68242e0b-8040-497e-b391-0bb5a109aa11",
   "metadata": {},
   "source": [
    "# Fine- truning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35967a42-d2c9-4143-b234-4093910ab208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from wesad_processing import *\n",
    "\n",
    "def load_wesad_dataset(root_dir, test_subject):\n",
    "    folder_ls = os.listdir(root_dir)\n",
    "    for i in folder_ls:\n",
    "        if i == \".ipynb_checkpoints\" or i == '.DS_Store':\n",
    "            print(f\"Removing {i}\")\n",
    "            folder_ls.remove(i)\n",
    "    \n",
    "    valid_ls = [test_subject]    \n",
    "    # Create the train list by excluding test_ls\n",
    "    train_ls = [subject for subject in folder_ls if subject not in valid_ls]\n",
    "    print(train_ls)\n",
    "    print(\"==========Loading Training set============\")\n",
    "    X_train, y_train = load_process_extract_ls(root_dir, train_ls,700, 10,10,True)\n",
    "    print(\"==========Loading Testing set============\")\n",
    "    X_test, y_test = load_process_extract_ls(root_dir,valid_ls,700, 10, 10,False)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "class ECGClassificationDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X  # shape: [N, L]\n",
    "        self.Y = Y  # shape: [N]\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.X[idx].flatten()\n",
    "        return torch.tensor(seq, dtype=torch.float32).unsqueeze(0), torch.tensor(self.Y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b4798e0-bee6-4a7f-a344-a08a5f2dcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=2):\n",
    "        super(DownstreamClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Freeze encoder parameters\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Classifier head after GAP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),  # Adjust 256 to match your encoder's output channels\n",
    "            nn.LeakyReLU(negative_slope = 0.01),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # If no mask given, create a zero-mask (no masking)\n",
    "        if mask is None:\n",
    "            mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "\n",
    "        x = x.squeeze(1)         # (B, L)\n",
    "        mask = mask.squeeze(1)   # (B, L)\n",
    "\n",
    "        # Pass through encoder\n",
    "        z = self.encoder(x, mask, mask)  # -> (B, C, L)\n",
    "\n",
    "        # Global Average Pooling over sequence dimension\n",
    "        z = F.adaptive_avg_pool1d(z, 1)  # -> (B, C, 1)\n",
    "        z = z.squeeze(-1)                # -> (B, C)\n",
    "\n",
    "        logits = self.classifier(z)      # -> (B, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e603be-961c-4d0f-a70f-7ba96f5ac813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    \n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in dataloader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            outputs = model(x_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(y_val.cpu().numpy())\n",
    "    \n",
    "    prec_temp = precision_score(all_val_labels, all_val_preds)\n",
    "    rec_temp = recall_score(all_val_labels, all_val_preds)\n",
    "    f1_temp = f1_score(all_val_labels, all_val_preds,average='macro')\n",
    "    acc_temp = accuracy_score(all_val_labels, all_val_preds)\n",
    "    \n",
    "    return prec_temp, rec_temp, f1_temp, acc_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a3b4ea5-ab7e-4bd1-bf18-0cee92f2dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_downstream_classifier(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=60,\n",
    "    device = 'cuda'\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "    model.to(device)\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_train_preds.extend(preds.cpu().numpy())\n",
    "            all_train_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_f1_macro = f1_score(all_train_labels, all_train_preds, average='macro')\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                outputs = model(x_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_val_preds.extend(preds.cpu().numpy())\n",
    "                all_val_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "        val_f1_macro = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}: \"\n",
    "              f\"Train Loss = {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Train Acc = {train_accuracy:.4f}, \"\n",
    "              f\"Train F1 = {train_f1_macro:.4f} | \"\n",
    "              f\"Val Loss = {avg_val_loss:.4f}, \"\n",
    "              f\"Val Acc = {val_accuracy:.4f}, \"\n",
    "              f\"Val F1 = {val_f1_macro:.4f}\")\n",
    "        \n",
    "    prec, rec, f1, acc = evaluate(model, test_loader, criterion, device)\n",
    "    return prec, rec, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43038e9a-aa1e-488a-921f-8b82fc68a1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing .DS_Store\n",
      "['S16', 'S13', 'S8', 'S4', 'S11', 'S15', 'S3', 'S2', 'S6', 'S7', 'S14', 'S10', 'S5', 'S17']\n",
      "==========Loading Training set============\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S16/S16.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S13/S13.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S8/S8.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S4/S4.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S11/S11.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S15/S15.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S3/S3.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S2/S2.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S6/S6.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S7/S7.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S14/S14.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S10/S10.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S5/S5.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S17/S17.pkl\n",
      "==========Loading Testing set============\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S9/S9.pkl\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_wesad_dataset('/home/van/NamQuang/Dataset/WESAD_LOSO', 'S9')\n",
    "train_dataset = ECGClassificationDataset(X_train, y_train)\n",
    "test_dataset = ECGClassificationDataset(X_test, y_test)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader_wesad = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader_wesad = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader_wesad = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "# Load full pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dae07e38-e412-42a0-8744-ef1a804395c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.5239, Train Acc = 0.7609, Train F1 = 0.4751 | Val Loss = 0.4766, Val Acc = 0.7758, Val F1 = 0.4848\n",
      "Epoch 1: Train Loss = 0.3903, Train Acc = 0.8309, Train F1 = 0.6811 | Val Loss = 0.3618, Val Acc = 0.8412, Val F1 = 0.7282\n",
      "Epoch 2: Train Loss = 0.3002, Train Acc = 0.8821, Train F1 = 0.7959 | Val Loss = 0.2743, Val Acc = 0.8873, Val F1 = 0.8175\n",
      "Epoch 3: Train Loss = 0.2414, Train Acc = 0.9067, Train F1 = 0.8467 | Val Loss = 0.2239, Val Acc = 0.8958, Val F1 = 0.8317\n",
      "Epoch 4: Train Loss = 0.1976, Train Acc = 0.9264, Train F1 = 0.8817 | Val Loss = 0.1774, Val Acc = 0.9345, Val F1 = 0.9037\n",
      "Epoch 5: Train Loss = 0.1695, Train Acc = 0.9385, Train F1 = 0.9040 | Val Loss = 0.1551, Val Acc = 0.9309, Val F1 = 0.8973\n",
      "Epoch 6: Train Loss = 0.1508, Train Acc = 0.9467, Train F1 = 0.9183 | Val Loss = 0.1424, Val Acc = 0.9382, Val F1 = 0.9085\n",
      "Epoch 7: Train Loss = 0.1401, Train Acc = 0.9485, Train F1 = 0.9214 | Val Loss = 0.1320, Val Acc = 0.9394, Val F1 = 0.9108\n",
      "Epoch 8: Train Loss = 0.1306, Train Acc = 0.9503, Train F1 = 0.9248 | Val Loss = 0.1218, Val Acc = 0.9442, Val F1 = 0.9205\n",
      "Epoch 9: Train Loss = 0.1216, Train Acc = 0.9536, Train F1 = 0.9299 | Val Loss = 0.1132, Val Acc = 0.9479, Val F1 = 0.9272\n",
      "Epoch 10: Train Loss = 0.1171, Train Acc = 0.9536, Train F1 = 0.9301 | Val Loss = 0.1093, Val Acc = 0.9467, Val F1 = 0.9261\n",
      "Epoch 11: Train Loss = 0.1126, Train Acc = 0.9570, Train F1 = 0.9358 | Val Loss = 0.1055, Val Acc = 0.9527, Val F1 = 0.9330\n",
      "Epoch 12: Train Loss = 0.1074, Train Acc = 0.9585, Train F1 = 0.9381 | Val Loss = 0.1192, Val Acc = 0.9479, Val F1 = 0.9235\n",
      "Epoch 13: Train Loss = 0.1060, Train Acc = 0.9594, Train F1 = 0.9391 | Val Loss = 0.0965, Val Acc = 0.9527, Val F1 = 0.9339\n",
      "Epoch 14: Train Loss = 0.0983, Train Acc = 0.9615, Train F1 = 0.9428 | Val Loss = 0.1008, Val Acc = 0.9576, Val F1 = 0.9384\n",
      "Epoch 15: Train Loss = 0.0938, Train Acc = 0.9652, Train F1 = 0.9480 | Val Loss = 0.0971, Val Acc = 0.9576, Val F1 = 0.9420\n",
      "Epoch 16: Train Loss = 0.0924, Train Acc = 0.9636, Train F1 = 0.9459 | Val Loss = 0.0885, Val Acc = 0.9552, Val F1 = 0.9373\n",
      "Epoch 17: Train Loss = 0.0885, Train Acc = 0.9661, Train F1 = 0.9496 | Val Loss = 0.0840, Val Acc = 0.9588, Val F1 = 0.9414\n",
      "Epoch 18: Train Loss = 0.0845, Train Acc = 0.9652, Train F1 = 0.9480 | Val Loss = 0.0827, Val Acc = 0.9612, Val F1 = 0.9447\n",
      "Epoch 19: Train Loss = 0.0824, Train Acc = 0.9676, Train F1 = 0.9518 | Val Loss = 0.0882, Val Acc = 0.9636, Val F1 = 0.9475\n",
      "Epoch 20: Train Loss = 0.0790, Train Acc = 0.9700, Train F1 = 0.9556 | Val Loss = 0.1035, Val Acc = 0.9552, Val F1 = 0.9341\n",
      "Epoch 21: Train Loss = 0.0811, Train Acc = 0.9721, Train F1 = 0.9587 | Val Loss = 0.0980, Val Acc = 0.9576, Val F1 = 0.9379\n",
      "Epoch 22: Train Loss = 0.0900, Train Acc = 0.9658, Train F1 = 0.9491 | Val Loss = 0.0762, Val Acc = 0.9624, Val F1 = 0.9479\n",
      "Epoch 23: Train Loss = 0.0734, Train Acc = 0.9718, Train F1 = 0.9583 | Val Loss = 0.0717, Val Acc = 0.9648, Val F1 = 0.9500\n",
      "Epoch 24: Train Loss = 0.0716, Train Acc = 0.9727, Train F1 = 0.9595 | Val Loss = 0.0746, Val Acc = 0.9648, Val F1 = 0.9496\n",
      "Epoch 25: Train Loss = 0.0712, Train Acc = 0.9733, Train F1 = 0.9604 | Val Loss = 0.0804, Val Acc = 0.9624, Val F1 = 0.9455\n",
      "Epoch 26: Train Loss = 0.0674, Train Acc = 0.9742, Train F1 = 0.9620 | Val Loss = 0.0666, Val Acc = 0.9673, Val F1 = 0.9536\n",
      "Epoch 27: Train Loss = 0.0633, Train Acc = 0.9758, Train F1 = 0.9641 | Val Loss = 0.0655, Val Acc = 0.9685, Val F1 = 0.9554\n",
      "Epoch 28: Train Loss = 0.0707, Train Acc = 0.9758, Train F1 = 0.9641 | Val Loss = 0.0807, Val Acc = 0.9612, Val F1 = 0.9436\n",
      "Epoch 29: Train Loss = 0.0630, Train Acc = 0.9785, Train F1 = 0.9681 | Val Loss = 0.0635, Val Acc = 0.9709, Val F1 = 0.9590\n",
      "Epoch 30: Train Loss = 0.0667, Train Acc = 0.9767, Train F1 = 0.9655 | Val Loss = 0.0639, Val Acc = 0.9685, Val F1 = 0.9563\n",
      "Epoch 31: Train Loss = 0.0620, Train Acc = 0.9779, Train F1 = 0.9674 | Val Loss = 0.0654, Val Acc = 0.9697, Val F1 = 0.9565\n",
      "Epoch 32: Train Loss = 0.0563, Train Acc = 0.9791, Train F1 = 0.9690 | Val Loss = 0.0638, Val Acc = 0.9697, Val F1 = 0.9567\n",
      "Epoch 33: Train Loss = 0.0588, Train Acc = 0.9812, Train F1 = 0.9722 | Val Loss = 0.0635, Val Acc = 0.9697, Val F1 = 0.9565\n",
      "Epoch 34: Train Loss = 0.0552, Train Acc = 0.9815, Train F1 = 0.9726 | Val Loss = 0.0581, Val Acc = 0.9733, Val F1 = 0.9621\n",
      "Epoch 35: Train Loss = 0.0603, Train Acc = 0.9800, Train F1 = 0.9705 | Val Loss = 0.0553, Val Acc = 0.9758, Val F1 = 0.9661\n",
      "Epoch 36: Train Loss = 0.0538, Train Acc = 0.9824, Train F1 = 0.9741 | Val Loss = 0.0546, Val Acc = 0.9782, Val F1 = 0.9692\n",
      "Epoch 37: Train Loss = 0.0549, Train Acc = 0.9824, Train F1 = 0.9739 | Val Loss = 0.0584, Val Acc = 0.9733, Val F1 = 0.9635\n",
      "Epoch 38: Train Loss = 0.0517, Train Acc = 0.9812, Train F1 = 0.9723 | Val Loss = 0.0520, Val Acc = 0.9782, Val F1 = 0.9692\n",
      "Epoch 39: Train Loss = 0.0521, Train Acc = 0.9833, Train F1 = 0.9754 | Val Loss = 0.0866, Val Acc = 0.9576, Val F1 = 0.9374\n",
      "Epoch 40: Train Loss = 0.0610, Train Acc = 0.9791, Train F1 = 0.9690 | Val Loss = 0.0521, Val Acc = 0.9794, Val F1 = 0.9710\n",
      "Epoch 41: Train Loss = 0.0525, Train Acc = 0.9824, Train F1 = 0.9741 | Val Loss = 0.0532, Val Acc = 0.9770, Val F1 = 0.9682\n",
      "Epoch 42: Train Loss = 0.0519, Train Acc = 0.9827, Train F1 = 0.9744 | Val Loss = 0.0590, Val Acc = 0.9721, Val F1 = 0.9600\n",
      "Epoch 43: Train Loss = 0.0525, Train Acc = 0.9809, Train F1 = 0.9717 | Val Loss = 0.0522, Val Acc = 0.9794, Val F1 = 0.9708\n",
      "Epoch 44: Train Loss = 0.0465, Train Acc = 0.9845, Train F1 = 0.9773 | Val Loss = 0.0679, Val Acc = 0.9685, Val F1 = 0.9544\n",
      "Epoch 45: Train Loss = 0.0490, Train Acc = 0.9842, Train F1 = 0.9766 | Val Loss = 0.0484, Val Acc = 0.9830, Val F1 = 0.9762\n",
      "Epoch 46: Train Loss = 0.0472, Train Acc = 0.9836, Train F1 = 0.9759 | Val Loss = 0.0506, Val Acc = 0.9794, Val F1 = 0.9708\n",
      "Epoch 47: Train Loss = 0.0478, Train Acc = 0.9818, Train F1 = 0.9733 | Val Loss = 0.0694, Val Acc = 0.9648, Val F1 = 0.9488\n",
      "Epoch 48: Train Loss = 0.0466, Train Acc = 0.9842, Train F1 = 0.9767 | Val Loss = 0.0461, Val Acc = 0.9818, Val F1 = 0.9744\n",
      "Epoch 49: Train Loss = 0.0500, Train Acc = 0.9818, Train F1 = 0.9732 | Val Loss = 0.0451, Val Acc = 0.9806, Val F1 = 0.9727\n",
      "Epoch 50: Train Loss = 0.0412, Train Acc = 0.9858, Train F1 = 0.9790 | Val Loss = 0.0466, Val Acc = 0.9818, Val F1 = 0.9747\n",
      "Epoch 51: Train Loss = 0.0422, Train Acc = 0.9867, Train F1 = 0.9803 | Val Loss = 0.0494, Val Acc = 0.9794, Val F1 = 0.9708\n",
      "Epoch 52: Train Loss = 0.0423, Train Acc = 0.9852, Train F1 = 0.9781 | Val Loss = 0.0505, Val Acc = 0.9758, Val F1 = 0.9654\n",
      "Epoch 53: Train Loss = 0.0399, Train Acc = 0.9861, Train F1 = 0.9795 | Val Loss = 0.0533, Val Acc = 0.9758, Val F1 = 0.9654\n",
      "Epoch 54: Train Loss = 0.0411, Train Acc = 0.9870, Train F1 = 0.9808 | Val Loss = 0.0411, Val Acc = 0.9855, Val F1 = 0.9796\n",
      "Epoch 55: Train Loss = 0.0466, Train Acc = 0.9842, Train F1 = 0.9767 | Val Loss = 0.0412, Val Acc = 0.9855, Val F1 = 0.9796\n",
      "Epoch 56: Train Loss = 0.0457, Train Acc = 0.9839, Train F1 = 0.9764 | Val Loss = 0.0444, Val Acc = 0.9806, Val F1 = 0.9725\n",
      "Epoch 57: Train Loss = 0.0378, Train Acc = 0.9882, Train F1 = 0.9825 | Val Loss = 0.0486, Val Acc = 0.9806, Val F1 = 0.9733\n",
      "Epoch 58: Train Loss = 0.0413, Train Acc = 0.9867, Train F1 = 0.9803 | Val Loss = 0.0409, Val Acc = 0.9830, Val F1 = 0.9761\n",
      "Epoch 59: Train Loss = 0.0394, Train Acc = 0.9873, Train F1 = 0.9813 | Val Loss = 0.0454, Val Acc = 0.9794, Val F1 = 0.9708\n",
      "Epoch 60: Train Loss = 0.0416, Train Acc = 0.9858, Train F1 = 0.9790 | Val Loss = 0.0518, Val Acc = 0.9758, Val F1 = 0.9653\n",
      "Epoch 61: Train Loss = 0.0377, Train Acc = 0.9891, Train F1 = 0.9839 | Val Loss = 0.0394, Val Acc = 0.9855, Val F1 = 0.9798\n",
      "Epoch 62: Train Loss = 0.0364, Train Acc = 0.9873, Train F1 = 0.9813 | Val Loss = 0.0475, Val Acc = 0.9782, Val F1 = 0.9689\n",
      "Epoch 63: Train Loss = 0.0353, Train Acc = 0.9894, Train F1 = 0.9844 | Val Loss = 0.0418, Val Acc = 0.9830, Val F1 = 0.9761\n",
      "Epoch 64: Train Loss = 0.0358, Train Acc = 0.9873, Train F1 = 0.9813 | Val Loss = 0.0477, Val Acc = 0.9782, Val F1 = 0.9689\n",
      "Epoch 65: Train Loss = 0.0367, Train Acc = 0.9870, Train F1 = 0.9808 | Val Loss = 0.0378, Val Acc = 0.9879, Val F1 = 0.9830\n",
      "Epoch 66: Train Loss = 0.0339, Train Acc = 0.9900, Train F1 = 0.9853 | Val Loss = 0.0436, Val Acc = 0.9794, Val F1 = 0.9708\n",
      "Epoch 67: Train Loss = 0.0315, Train Acc = 0.9906, Train F1 = 0.9862 | Val Loss = 0.0404, Val Acc = 0.9830, Val F1 = 0.9761\n",
      "Epoch 68: Train Loss = 0.0321, Train Acc = 0.9903, Train F1 = 0.9857 | Val Loss = 0.0371, Val Acc = 0.9867, Val F1 = 0.9813\n",
      "Epoch 69: Train Loss = 0.0364, Train Acc = 0.9903, Train F1 = 0.9858 | Val Loss = 0.0400, Val Acc = 0.9842, Val F1 = 0.9778\n",
      "Epoch 70: Train Loss = 0.0327, Train Acc = 0.9903, Train F1 = 0.9857 | Val Loss = 0.0373, Val Acc = 0.9867, Val F1 = 0.9813\n",
      "Epoch 71: Train Loss = 0.0340, Train Acc = 0.9885, Train F1 = 0.9830 | Val Loss = 0.0347, Val Acc = 0.9867, Val F1 = 0.9813\n",
      "Epoch 72: Train Loss = 0.0295, Train Acc = 0.9897, Train F1 = 0.9848 | Val Loss = 0.0374, Val Acc = 0.9891, Val F1 = 0.9850\n",
      "Epoch 73: Train Loss = 0.0338, Train Acc = 0.9897, Train F1 = 0.9848 | Val Loss = 0.0343, Val Acc = 0.9879, Val F1 = 0.9831\n",
      "Epoch 74: Train Loss = 0.0333, Train Acc = 0.9891, Train F1 = 0.9840 | Val Loss = 0.0357, Val Acc = 0.9855, Val F1 = 0.9796\n",
      "Epoch 75: Train Loss = 0.0349, Train Acc = 0.9882, Train F1 = 0.9826 | Val Loss = 0.0342, Val Acc = 0.9879, Val F1 = 0.9833\n",
      "Epoch 76: Train Loss = 0.0294, Train Acc = 0.9924, Train F1 = 0.9889 | Val Loss = 0.0337, Val Acc = 0.9867, Val F1 = 0.9814\n",
      "Epoch 77: Train Loss = 0.0297, Train Acc = 0.9918, Train F1 = 0.9880 | Val Loss = 0.0359, Val Acc = 0.9855, Val F1 = 0.9796\n",
      "Epoch 78: Train Loss = 0.0357, Train Acc = 0.9888, Train F1 = 0.9835 | Val Loss = 0.0343, Val Acc = 0.9879, Val F1 = 0.9830\n",
      "Epoch 79: Train Loss = 0.0282, Train Acc = 0.9918, Train F1 = 0.9879 | Val Loss = 0.0360, Val Acc = 0.9867, Val F1 = 0.9812\n",
      "Epoch 80: Train Loss = 0.0317, Train Acc = 0.9900, Train F1 = 0.9853 | Val Loss = 0.0393, Val Acc = 0.9891, Val F1 = 0.9850\n",
      "Epoch 81: Train Loss = 0.0324, Train Acc = 0.9891, Train F1 = 0.9840 | Val Loss = 0.0537, Val Acc = 0.9733, Val F1 = 0.9617\n",
      "Epoch 82: Train Loss = 0.0290, Train Acc = 0.9927, Train F1 = 0.9893 | Val Loss = 0.0321, Val Acc = 0.9891, Val F1 = 0.9849\n",
      "Epoch 83: Train Loss = 0.0276, Train Acc = 0.9915, Train F1 = 0.9875 | Val Loss = 0.0302, Val Acc = 0.9891, Val F1 = 0.9848\n",
      "Epoch 84: Train Loss = 0.0273, Train Acc = 0.9924, Train F1 = 0.9889 | Val Loss = 0.0323, Val Acc = 0.9842, Val F1 = 0.9778\n",
      "Epoch 85: Train Loss = 0.0268, Train Acc = 0.9930, Train F1 = 0.9897 | Val Loss = 0.0352, Val Acc = 0.9855, Val F1 = 0.9795\n",
      "Epoch 86: Train Loss = 0.0296, Train Acc = 0.9906, Train F1 = 0.9861 | Val Loss = 0.0336, Val Acc = 0.9879, Val F1 = 0.9831\n",
      "Epoch 87: Train Loss = 0.0380, Train Acc = 0.9855, Train F1 = 0.9786 | Val Loss = 0.0436, Val Acc = 0.9782, Val F1 = 0.9689\n",
      "Epoch 88: Train Loss = 0.0298, Train Acc = 0.9906, Train F1 = 0.9861 | Val Loss = 0.0310, Val Acc = 0.9867, Val F1 = 0.9813\n",
      "Epoch 89: Train Loss = 0.0274, Train Acc = 0.9912, Train F1 = 0.9871 | Val Loss = 0.0357, Val Acc = 0.9903, Val F1 = 0.9867\n",
      "Epoch 90: Train Loss = 0.0268, Train Acc = 0.9921, Train F1 = 0.9884 | Val Loss = 0.0313, Val Acc = 0.9879, Val F1 = 0.9830\n",
      "Epoch 91: Train Loss = 0.0243, Train Acc = 0.9933, Train F1 = 0.9902 | Val Loss = 0.0386, Val Acc = 0.9842, Val F1 = 0.9777\n",
      "Epoch 92: Train Loss = 0.0255, Train Acc = 0.9930, Train F1 = 0.9897 | Val Loss = 0.0328, Val Acc = 0.9855, Val F1 = 0.9795\n",
      "Epoch 93: Train Loss = 0.0258, Train Acc = 0.9921, Train F1 = 0.9884 | Val Loss = 0.0467, Val Acc = 0.9782, Val F1 = 0.9689\n",
      "Epoch 94: Train Loss = 0.0249, Train Acc = 0.9933, Train F1 = 0.9902 | Val Loss = 0.0305, Val Acc = 0.9879, Val F1 = 0.9830\n",
      "Epoch 95: Train Loss = 0.0271, Train Acc = 0.9915, Train F1 = 0.9875 | Val Loss = 0.0265, Val Acc = 0.9927, Val F1 = 0.9899\n",
      "Epoch 96: Train Loss = 0.0272, Train Acc = 0.9921, Train F1 = 0.9884 | Val Loss = 0.0325, Val Acc = 0.9891, Val F1 = 0.9847\n",
      "Epoch 97: Train Loss = 0.0274, Train Acc = 0.9909, Train F1 = 0.9866 | Val Loss = 0.0312, Val Acc = 0.9879, Val F1 = 0.9833\n",
      "Epoch 98: Train Loss = 0.0231, Train Acc = 0.9945, Train F1 = 0.9920 | Val Loss = 0.0271, Val Acc = 0.9903, Val F1 = 0.9865\n",
      "Epoch 99: Train Loss = 0.0232, Train Acc = 0.9927, Train F1 = 0.9893 | Val Loss = 0.0286, Val Acc = 0.9891, Val F1 = 0.9848\n"
     ]
    }
   ],
   "source": [
    "auto_model = MAE1D_Mask()\n",
    "auto_model.load_state_dict(torch.load('runs/seq1000_rpeak6_1811_27082025/model_1000_6_1811_27082025.pth', weights_only = True))\n",
    "encoder = auto_model.encoder\n",
    "model = DownstreamClassifier(encoder)\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "prec_, rec_, f1_, acc_ =  train_evaluate_downstream_classifier(\n",
    "    model,\n",
    "    train_loader_wesad,\n",
    "    val_loader_wesad,\n",
    "    test_loader_wesad,\n",
    "    num_epochs=100,\n",
    "    device = 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb226e55-71ce-41e9-9695-316760d5e795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4028985507246377"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2978324-af13-4eef-9da2-7207771d8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loso_training(root_dir, sample_rate, test_size, stride, filename, load_file):\n",
    "    prec, rec, acc, f1 = [], [], [], []\n",
    "    folder_ls = [f for f in os.listdir(root_dir) if f not in (\".ipynb_checkpoints\", \".DS_Store\")]\n",
    "    start_index = np.arange(0, len(folder_ls) - test_size + 1, stride)\n",
    "\n",
    "    # Load pretrained model once\n",
    "    auto_model = MAE1D_Mask()\n",
    "    auto_model.load_state_dict(torch.load(\n",
    "        'runs/seq1000_rpeak6_1811_27082025/model_1000_6_1811_27082025.pth', \n",
    "        weights_only=True\n",
    "    ))\n",
    "    encoder = auto_model.encoder\n",
    "\n",
    "    for start in start_index:\n",
    "        subj = folder_ls[start]\n",
    "        print(f\"***** Loop {start}: {subj} *****\")\n",
    "\n",
    "        # Load dataset\n",
    "        X_train, X_test, y_train, y_test = load_wesad_dataset(root_dir, subj)\n",
    "        train_dataset = ECGClassificationDataset(X_train, y_train)\n",
    "        test_dataset = ECGClassificationDataset(X_test, y_test)\n",
    "\n",
    "        # Split train/val\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "        loaders = {\n",
    "            'train': DataLoader(train_dataset, batch_size=128, shuffle=True),\n",
    "            'val': DataLoader(val_dataset, batch_size=128, shuffle=False),\n",
    "            'test': DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "        }\n",
    "\n",
    "        # Train and evaluate downstream classifier\n",
    "        model = DownstreamClassifier(encoder)\n",
    "        prec_, rec_, f1_, acc_ = train_evaluate_downstream_classifier(\n",
    "            model, loaders['train'], loaders['val'], loaders['test'], num_epochs=100, device='cuda'\n",
    "        )\n",
    "        prec.append(prec_); rec.append(rec_); f1.append(f1_); acc.append(acc_)\n",
    "        print([start, acc_, f1_, rec_, prec_])\n",
    "\n",
    "        # Save iteration results\n",
    "        with open(filename, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow([start, acc_, f1_, rec_, prec_, subj])\n",
    "\n",
    "    # Save mean results\n",
    "    with open(filename, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([np.mean(acc), np.mean(f1), np.mean(prec), np.mean(rec)])\n",
    "\n",
    "    print(f\"Accuracy: {np.mean(acc)}, F1: {np.mean(f1)}, Precision: {np.mean(prec)}, Recall: {np.mean(rec)}\")\n",
    "    return acc, f1, rec, prec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb4065-67e5-47b2-bf1e-3a4193d6c223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing .DS_Store\n",
      "************************************\n",
      "*The loop 0                 *\n",
      "************************************\n",
      "S16\n",
      "Removing .DS_Store\n",
      "['S9', 'S13', 'S8', 'S4', 'S11', 'S15', 'S3', 'S2', 'S6', 'S7', 'S14', 'S10', 'S5', 'S17']\n",
      "==========Loading Training set============\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S9/S9.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S13/S13.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S8/S8.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S4/S4.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S11/S11.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S15/S15.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S3/S3.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S2/S2.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S6/S6.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S7/S7.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S14/S14.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S10/S10.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S5/S5.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S17/S17.pkl\n",
      "==========Loading Testing set============\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S16/S16.pkl\n",
      "Epoch 0: Train Loss = 0.5426, Train Acc = 0.7589, Train F1 = 0.4803 | Val Loss = 0.5098, Val Acc = 0.7576, Val F1 = 0.4588\n",
      "Epoch 1: Train Loss = 0.4332, Train Acc = 0.8062, Train F1 = 0.5628 | Val Loss = 0.4456, Val Acc = 0.7721, Val F1 = 0.5812\n",
      "Epoch 2: Train Loss = 0.3712, Train Acc = 0.8402, Train F1 = 0.6881 | Val Loss = 0.3925, Val Acc = 0.8267, Val F1 = 0.6967\n",
      "Epoch 3: Train Loss = 0.3145, Train Acc = 0.8754, Train F1 = 0.7726 | Val Loss = 0.3260, Val Acc = 0.8691, Val F1 = 0.7991\n",
      "Epoch 4: Train Loss = 0.2670, Train Acc = 0.8993, Train F1 = 0.8266 | Val Loss = 0.2908, Val Acc = 0.8776, Val F1 = 0.8074\n",
      "Epoch 5: Train Loss = 0.2299, Train Acc = 0.9175, Train F1 = 0.8627 | Val Loss = 0.2393, Val Acc = 0.9055, Val F1 = 0.8636\n",
      "Epoch 6: Train Loss = 0.1967, Train Acc = 0.9306, Train F1 = 0.8880 | Val Loss = 0.2180, Val Acc = 0.9139, Val F1 = 0.8822\n",
      "Epoch 7: Train Loss = 0.1740, Train Acc = 0.9348, Train F1 = 0.8969 | Val Loss = 0.1981, Val Acc = 0.9236, Val F1 = 0.8969\n",
      "Epoch 8: Train Loss = 0.1612, Train Acc = 0.9403, Train F1 = 0.9068 | Val Loss = 0.1814, Val Acc = 0.9236, Val F1 = 0.8929\n",
      "Epoch 9: Train Loss = 0.1485, Train Acc = 0.9397, Train F1 = 0.9068 | Val Loss = 0.1932, Val Acc = 0.9200, Val F1 = 0.8828\n",
      "Epoch 10: Train Loss = 0.1417, Train Acc = 0.9433, Train F1 = 0.9119 | Val Loss = 0.1706, Val Acc = 0.9297, Val F1 = 0.9016\n",
      "Epoch 11: Train Loss = 0.1291, Train Acc = 0.9533, Train F1 = 0.9283 | Val Loss = 0.1604, Val Acc = 0.9382, Val F1 = 0.9168\n",
      "Epoch 12: Train Loss = 0.1232, Train Acc = 0.9494, Train F1 = 0.9227 | Val Loss = 0.1602, Val Acc = 0.9394, Val F1 = 0.9148\n",
      "Epoch 13: Train Loss = 0.1179, Train Acc = 0.9515, Train F1 = 0.9262 | Val Loss = 0.1478, Val Acc = 0.9418, Val F1 = 0.9203\n",
      "Epoch 14: Train Loss = 0.1139, Train Acc = 0.9524, Train F1 = 0.9274 | Val Loss = 0.1449, Val Acc = 0.9418, Val F1 = 0.9219\n",
      "Epoch 15: Train Loss = 0.1190, Train Acc = 0.9527, Train F1 = 0.9288 | Val Loss = 0.1615, Val Acc = 0.9358, Val F1 = 0.9085\n",
      "Epoch 16: Train Loss = 0.1192, Train Acc = 0.9545, Train F1 = 0.9306 | Val Loss = 0.1380, Val Acc = 0.9467, Val F1 = 0.9297\n",
      "Epoch 17: Train Loss = 0.1089, Train Acc = 0.9572, Train F1 = 0.9354 | Val Loss = 0.1366, Val Acc = 0.9442, Val F1 = 0.9256\n",
      "Epoch 18: Train Loss = 0.0974, Train Acc = 0.9624, Train F1 = 0.9432 | Val Loss = 0.1262, Val Acc = 0.9491, Val F1 = 0.9302\n",
      "Epoch 19: Train Loss = 0.0941, Train Acc = 0.9624, Train F1 = 0.9434 | Val Loss = 0.1265, Val Acc = 0.9515, Val F1 = 0.9331\n",
      "Epoch 20: Train Loss = 0.0913, Train Acc = 0.9636, Train F1 = 0.9451 | Val Loss = 0.1188, Val Acc = 0.9552, Val F1 = 0.9393\n",
      "Epoch 21: Train Loss = 0.0920, Train Acc = 0.9630, Train F1 = 0.9445 | Val Loss = 0.1245, Val Acc = 0.9552, Val F1 = 0.9404\n",
      "Epoch 22: Train Loss = 0.0862, Train Acc = 0.9682, Train F1 = 0.9521 | Val Loss = 0.1193, Val Acc = 0.9539, Val F1 = 0.9369\n",
      "Epoch 23: Train Loss = 0.0848, Train Acc = 0.9691, Train F1 = 0.9534 | Val Loss = 0.1107, Val Acc = 0.9600, Val F1 = 0.9462\n",
      "Epoch 24: Train Loss = 0.0821, Train Acc = 0.9709, Train F1 = 0.9561 | Val Loss = 0.1088, Val Acc = 0.9588, Val F1 = 0.9445\n",
      "Epoch 25: Train Loss = 0.0798, Train Acc = 0.9676, Train F1 = 0.9512 | Val Loss = 0.1041, Val Acc = 0.9636, Val F1 = 0.9512\n",
      "Epoch 26: Train Loss = 0.0780, Train Acc = 0.9688, Train F1 = 0.9530 | Val Loss = 0.1141, Val Acc = 0.9588, Val F1 = 0.9427\n",
      "Epoch 27: Train Loss = 0.0764, Train Acc = 0.9691, Train F1 = 0.9534 | Val Loss = 0.1044, Val Acc = 0.9612, Val F1 = 0.9481\n",
      "Epoch 28: Train Loss = 0.0755, Train Acc = 0.9700, Train F1 = 0.9549 | Val Loss = 0.1014, Val Acc = 0.9624, Val F1 = 0.9499\n",
      "Epoch 29: Train Loss = 0.0793, Train Acc = 0.9682, Train F1 = 0.9521 | Val Loss = 0.0992, Val Acc = 0.9636, Val F1 = 0.9503\n",
      "Epoch 30: Train Loss = 0.0717, Train Acc = 0.9739, Train F1 = 0.9608 | Val Loss = 0.1016, Val Acc = 0.9576, Val F1 = 0.9420\n",
      "Epoch 31: Train Loss = 0.0696, Train Acc = 0.9727, Train F1 = 0.9590 | Val Loss = 0.1155, Val Acc = 0.9527, Val F1 = 0.9339\n",
      "Epoch 32: Train Loss = 0.0671, Train Acc = 0.9763, Train F1 = 0.9646 | Val Loss = 0.0915, Val Acc = 0.9685, Val F1 = 0.9577\n",
      "Epoch 33: Train Loss = 0.0694, Train Acc = 0.9730, Train F1 = 0.9596 | Val Loss = 0.0936, Val Acc = 0.9685, Val F1 = 0.9581\n",
      "Epoch 34: Train Loss = 0.0637, Train Acc = 0.9754, Train F1 = 0.9629 | Val Loss = 0.0893, Val Acc = 0.9709, Val F1 = 0.9611\n",
      "Epoch 35: Train Loss = 0.0623, Train Acc = 0.9767, Train F1 = 0.9651 | Val Loss = 0.0916, Val Acc = 0.9661, Val F1 = 0.9536\n",
      "Epoch 36: Train Loss = 0.0631, Train Acc = 0.9757, Train F1 = 0.9636 | Val Loss = 0.0970, Val Acc = 0.9624, Val F1 = 0.9479\n",
      "Epoch 37: Train Loss = 0.0680, Train Acc = 0.9748, Train F1 = 0.9621 | Val Loss = 0.0929, Val Acc = 0.9648, Val F1 = 0.9535\n",
      "Epoch 38: Train Loss = 0.0694, Train Acc = 0.9727, Train F1 = 0.9593 | Val Loss = 0.0865, Val Acc = 0.9673, Val F1 = 0.9557\n",
      "Epoch 39: Train Loss = 0.0580, Train Acc = 0.9782, Train F1 = 0.9672 | Val Loss = 0.0916, Val Acc = 0.9636, Val F1 = 0.9496\n",
      "Epoch 40: Train Loss = 0.0612, Train Acc = 0.9757, Train F1 = 0.9635 | Val Loss = 0.0812, Val Acc = 0.9733, Val F1 = 0.9639\n",
      "Epoch 41: Train Loss = 0.0559, Train Acc = 0.9797, Train F1 = 0.9697 | Val Loss = 0.0880, Val Acc = 0.9636, Val F1 = 0.9500\n",
      "Epoch 42: Train Loss = 0.0613, Train Acc = 0.9763, Train F1 = 0.9645 | Val Loss = 0.0960, Val Acc = 0.9600, Val F1 = 0.9485\n",
      "Epoch 43: Train Loss = 0.0601, Train Acc = 0.9751, Train F1 = 0.9627 | Val Loss = 0.0913, Val Acc = 0.9636, Val F1 = 0.9500\n",
      "Epoch 44: Train Loss = 0.0585, Train Acc = 0.9751, Train F1 = 0.9627 | Val Loss = 0.0816, Val Acc = 0.9721, Val F1 = 0.9630\n",
      "Epoch 45: Train Loss = 0.0543, Train Acc = 0.9806, Train F1 = 0.9709 | Val Loss = 0.0767, Val Acc = 0.9745, Val F1 = 0.9655\n",
      "Epoch 46: Train Loss = 0.0529, Train Acc = 0.9809, Train F1 = 0.9715 | Val Loss = 0.0799, Val Acc = 0.9673, Val F1 = 0.9549\n",
      "Epoch 47: Train Loss = 0.0541, Train Acc = 0.9803, Train F1 = 0.9705 | Val Loss = 0.0768, Val Acc = 0.9709, Val F1 = 0.9605\n",
      "Epoch 48: Train Loss = 0.0552, Train Acc = 0.9785, Train F1 = 0.9677 | Val Loss = 0.0858, Val Acc = 0.9685, Val F1 = 0.9587\n",
      "Epoch 49: Train Loss = 0.0555, Train Acc = 0.9788, Train F1 = 0.9684 | Val Loss = 0.0772, Val Acc = 0.9685, Val F1 = 0.9568\n",
      "Epoch 50: Train Loss = 0.0535, Train Acc = 0.9812, Train F1 = 0.9718 | Val Loss = 0.0753, Val Acc = 0.9709, Val F1 = 0.9603\n",
      "Epoch 51: Train Loss = 0.0508, Train Acc = 0.9806, Train F1 = 0.9710 | Val Loss = 0.0742, Val Acc = 0.9733, Val F1 = 0.9636\n",
      "Epoch 52: Train Loss = 0.0500, Train Acc = 0.9812, Train F1 = 0.9719 | Val Loss = 0.0707, Val Acc = 0.9745, Val F1 = 0.9656\n",
      "Epoch 53: Train Loss = 0.0516, Train Acc = 0.9815, Train F1 = 0.9724 | Val Loss = 0.0732, Val Acc = 0.9770, Val F1 = 0.9695\n",
      "Epoch 54: Train Loss = 0.0475, Train Acc = 0.9827, Train F1 = 0.9741 | Val Loss = 0.0697, Val Acc = 0.9758, Val F1 = 0.9674\n",
      "Epoch 55: Train Loss = 0.0453, Train Acc = 0.9845, Train F1 = 0.9768 | Val Loss = 0.0744, Val Acc = 0.9758, Val F1 = 0.9682\n",
      "Epoch 56: Train Loss = 0.0479, Train Acc = 0.9821, Train F1 = 0.9733 | Val Loss = 0.0809, Val Acc = 0.9745, Val F1 = 0.9666\n",
      "Epoch 57: Train Loss = 0.0438, Train Acc = 0.9848, Train F1 = 0.9774 | Val Loss = 0.0687, Val Acc = 0.9745, Val F1 = 0.9654\n",
      "Epoch 58: Train Loss = 0.0421, Train Acc = 0.9867, Train F1 = 0.9801 | Val Loss = 0.0685, Val Acc = 0.9758, Val F1 = 0.9672\n",
      "Epoch 59: Train Loss = 0.0421, Train Acc = 0.9848, Train F1 = 0.9773 | Val Loss = 0.0711, Val Acc = 0.9697, Val F1 = 0.9584\n",
      "Epoch 60: Train Loss = 0.0426, Train Acc = 0.9870, Train F1 = 0.9805 | Val Loss = 0.0632, Val Acc = 0.9806, Val F1 = 0.9740\n",
      "Epoch 61: Train Loss = 0.0425, Train Acc = 0.9861, Train F1 = 0.9792 | Val Loss = 0.0648, Val Acc = 0.9782, Val F1 = 0.9710\n",
      "Epoch 62: Train Loss = 0.0421, Train Acc = 0.9842, Train F1 = 0.9764 | Val Loss = 0.0648, Val Acc = 0.9830, Val F1 = 0.9774\n",
      "Epoch 63: Train Loss = 0.0427, Train Acc = 0.9848, Train F1 = 0.9774 | Val Loss = 0.0717, Val Acc = 0.9733, Val F1 = 0.9636\n",
      "Epoch 64: Train Loss = 0.0435, Train Acc = 0.9842, Train F1 = 0.9765 | Val Loss = 0.0723, Val Acc = 0.9709, Val F1 = 0.9603\n",
      "Epoch 65: Train Loss = 0.0420, Train Acc = 0.9833, Train F1 = 0.9751 | Val Loss = 0.0636, Val Acc = 0.9818, Val F1 = 0.9759\n",
      "Epoch 66: Train Loss = 0.0441, Train Acc = 0.9845, Train F1 = 0.9769 | Val Loss = 0.0625, Val Acc = 0.9806, Val F1 = 0.9744\n",
      "Epoch 67: Train Loss = 0.0422, Train Acc = 0.9836, Train F1 = 0.9755 | Val Loss = 0.0699, Val Acc = 0.9733, Val F1 = 0.9636\n",
      "Epoch 68: Train Loss = 0.0378, Train Acc = 0.9864, Train F1 = 0.9797 | Val Loss = 0.0605, Val Acc = 0.9794, Val F1 = 0.9726\n",
      "Epoch 69: Train Loss = 0.0416, Train Acc = 0.9861, Train F1 = 0.9792 | Val Loss = 0.0740, Val Acc = 0.9758, Val F1 = 0.9682\n",
      "Epoch 70: Train Loss = 0.0379, Train Acc = 0.9870, Train F1 = 0.9805 | Val Loss = 0.0634, Val Acc = 0.9770, Val F1 = 0.9687\n",
      "Epoch 71: Train Loss = 0.0418, Train Acc = 0.9851, Train F1 = 0.9778 | Val Loss = 0.0643, Val Acc = 0.9782, Val F1 = 0.9713\n",
      "Epoch 72: Train Loss = 0.0385, Train Acc = 0.9864, Train F1 = 0.9796 | Val Loss = 0.0581, Val Acc = 0.9794, Val F1 = 0.9721\n",
      "Epoch 73: Train Loss = 0.0364, Train Acc = 0.9882, Train F1 = 0.9823 | Val Loss = 0.0631, Val Acc = 0.9782, Val F1 = 0.9704\n",
      "Epoch 74: Train Loss = 0.0372, Train Acc = 0.9882, Train F1 = 0.9823 | Val Loss = 0.0576, Val Acc = 0.9782, Val F1 = 0.9707\n",
      "Epoch 75: Train Loss = 0.0360, Train Acc = 0.9879, Train F1 = 0.9819 | Val Loss = 0.0601, Val Acc = 0.9818, Val F1 = 0.9759\n",
      "Epoch 76: Train Loss = 0.0389, Train Acc = 0.9857, Train F1 = 0.9787 | Val Loss = 0.0586, Val Acc = 0.9794, Val F1 = 0.9721\n",
      "Epoch 77: Train Loss = 0.0366, Train Acc = 0.9888, Train F1 = 0.9833 | Val Loss = 0.0582, Val Acc = 0.9782, Val F1 = 0.9708\n",
      "Epoch 78: Train Loss = 0.0358, Train Acc = 0.9894, Train F1 = 0.9842 | Val Loss = 0.0618, Val Acc = 0.9758, Val F1 = 0.9675\n",
      "Epoch 79: Train Loss = 0.0414, Train Acc = 0.9842, Train F1 = 0.9764 | Val Loss = 0.0667, Val Acc = 0.9745, Val F1 = 0.9665\n",
      "Epoch 80: Train Loss = 0.0348, Train Acc = 0.9888, Train F1 = 0.9833 | Val Loss = 0.0546, Val Acc = 0.9794, Val F1 = 0.9722\n",
      "Epoch 81: Train Loss = 0.0351, Train Acc = 0.9864, Train F1 = 0.9797 | Val Loss = 0.0556, Val Acc = 0.9818, Val F1 = 0.9757\n",
      "Epoch 82: Train Loss = 0.0320, Train Acc = 0.9900, Train F1 = 0.9851 | Val Loss = 0.0649, Val Acc = 0.9758, Val F1 = 0.9671\n",
      "Epoch 83: Train Loss = 0.0317, Train Acc = 0.9900, Train F1 = 0.9851 | Val Loss = 0.0570, Val Acc = 0.9794, Val F1 = 0.9728\n",
      "Epoch 84: Train Loss = 0.0315, Train Acc = 0.9909, Train F1 = 0.9865 | Val Loss = 0.0553, Val Acc = 0.9818, Val F1 = 0.9755\n",
      "Epoch 85: Train Loss = 0.0310, Train Acc = 0.9909, Train F1 = 0.9865 | Val Loss = 0.0592, Val Acc = 0.9770, Val F1 = 0.9690\n",
      "Epoch 86: Train Loss = 0.0310, Train Acc = 0.9900, Train F1 = 0.9851 | Val Loss = 0.0538, Val Acc = 0.9842, Val F1 = 0.9790\n",
      "Epoch 87: Train Loss = 0.0326, Train Acc = 0.9888, Train F1 = 0.9833 | Val Loss = 0.0524, Val Acc = 0.9830, Val F1 = 0.9774\n",
      "Epoch 88: Train Loss = 0.0417, Train Acc = 0.9845, Train F1 = 0.9769 | Val Loss = 0.0676, Val Acc = 0.9733, Val F1 = 0.9635\n",
      "Epoch 89: Train Loss = 0.0385, Train Acc = 0.9854, Train F1 = 0.9783 | Val Loss = 0.0607, Val Acc = 0.9745, Val F1 = 0.9666\n",
      "Epoch 90: Train Loss = 0.0324, Train Acc = 0.9894, Train F1 = 0.9842 | Val Loss = 0.0550, Val Acc = 0.9830, Val F1 = 0.9774\n",
      "Epoch 91: Train Loss = 0.0360, Train Acc = 0.9885, Train F1 = 0.9828 | Val Loss = 0.0596, Val Acc = 0.9782, Val F1 = 0.9704\n",
      "Epoch 92: Train Loss = 0.0313, Train Acc = 0.9900, Train F1 = 0.9851 | Val Loss = 0.0643, Val Acc = 0.9745, Val F1 = 0.9666\n",
      "Epoch 93: Train Loss = 0.0325, Train Acc = 0.9906, Train F1 = 0.9860 | Val Loss = 0.0517, Val Acc = 0.9806, Val F1 = 0.9743\n",
      "Epoch 94: Train Loss = 0.0317, Train Acc = 0.9894, Train F1 = 0.9842 | Val Loss = 0.0514, Val Acc = 0.9818, Val F1 = 0.9759\n",
      "Epoch 95: Train Loss = 0.0330, Train Acc = 0.9876, Train F1 = 0.9815 | Val Loss = 0.0564, Val Acc = 0.9806, Val F1 = 0.9737\n",
      "Epoch 96: Train Loss = 0.0313, Train Acc = 0.9900, Train F1 = 0.9851 | Val Loss = 0.0536, Val Acc = 0.9794, Val F1 = 0.9727\n",
      "Epoch 97: Train Loss = 0.0307, Train Acc = 0.9897, Train F1 = 0.9846 | Val Loss = 0.0498, Val Acc = 0.9830, Val F1 = 0.9771\n",
      "Epoch 98: Train Loss = 0.0270, Train Acc = 0.9915, Train F1 = 0.9874 | Val Loss = 0.0481, Val Acc = 0.9830, Val F1 = 0.9773\n",
      "Epoch 99: Train Loss = 0.0272, Train Acc = 0.9918, Train F1 = 0.9878 | Val Loss = 0.0477, Val Acc = 0.9818, Val F1 = 0.9759\n",
      "[np.int64(0), 0.8716216216216216, 0.844296788482835, 1.0, 0.638095238095238]\n",
      "************************************\n",
      "*The loop 1                 *\n",
      "************************************\n",
      "S9\n",
      "Removing .DS_Store\n",
      "['S16', 'S13', 'S8', 'S4', 'S11', 'S15', 'S3', 'S2', 'S6', 'S7', 'S14', 'S10', 'S5', 'S17']\n",
      "==========Loading Training set============\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S16/S16.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S13/S13.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S8/S8.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S4/S4.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S11/S11.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S15/S15.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S3/S3.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S2/S2.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S6/S6.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S7/S7.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S14/S14.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S10/S10.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S5/S5.pkl\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S17/S17.pkl\n",
      "==========Loading Testing set============\n",
      "minmax\n",
      "5-15\n",
      "Working with: /home/van/NamQuang/Dataset/WESAD_LOSO/S9/S9.pkl\n",
      "Epoch 0: Train Loss = 0.4880, Train Acc = 0.7891, Train F1 = 0.5071 | Val Loss = 0.4147, Val Acc = 0.8194, Val F1 = 0.6711\n",
      "Epoch 1: Train Loss = 0.3524, Train Acc = 0.8573, Train F1 = 0.7433 | Val Loss = 0.3127, Val Acc = 0.8752, Val F1 = 0.7874\n",
      "Epoch 2: Train Loss = 0.2657, Train Acc = 0.8997, Train F1 = 0.8304 | Val Loss = 0.2444, Val Acc = 0.9079, Val F1 = 0.8491\n",
      "Epoch 3: Train Loss = 0.2087, Train Acc = 0.9218, Train F1 = 0.8738 | Val Loss = 0.2012, Val Acc = 0.9248, Val F1 = 0.8826\n",
      "Epoch 4: Train Loss = 0.1742, Train Acc = 0.9355, Train F1 = 0.8994 | Val Loss = 0.1708, Val Acc = 0.9406, Val F1 = 0.9124\n",
      "Epoch 5: Train Loss = 0.1510, Train Acc = 0.9458, Train F1 = 0.9177 | Val Loss = 0.1549, Val Acc = 0.9418, Val F1 = 0.9164\n",
      "Epoch 6: Train Loss = 0.1377, Train Acc = 0.9491, Train F1 = 0.9230 | Val Loss = 0.1408, Val Acc = 0.9479, Val F1 = 0.9244\n",
      "Epoch 7: Train Loss = 0.1276, Train Acc = 0.9515, Train F1 = 0.9275 | Val Loss = 0.1331, Val Acc = 0.9442, Val F1 = 0.9196\n",
      "Epoch 8: Train Loss = 0.1174, Train Acc = 0.9545, Train F1 = 0.9323 | Val Loss = 0.1292, Val Acc = 0.9479, Val F1 = 0.9261\n",
      "Epoch 9: Train Loss = 0.1116, Train Acc = 0.9524, Train F1 = 0.9294 | Val Loss = 0.1253, Val Acc = 0.9491, Val F1 = 0.9248\n",
      "Epoch 10: Train Loss = 0.1111, Train Acc = 0.9582, Train F1 = 0.9375 | Val Loss = 0.1218, Val Acc = 0.9515, Val F1 = 0.9319\n",
      "Epoch 11: Train Loss = 0.1078, Train Acc = 0.9555, Train F1 = 0.9339 | Val Loss = 0.1157, Val Acc = 0.9515, Val F1 = 0.9314\n",
      "Epoch 12: Train Loss = 0.1022, Train Acc = 0.9600, Train F1 = 0.9410 | Val Loss = 0.1205, Val Acc = 0.9503, Val F1 = 0.9255\n",
      "Epoch 13: Train Loss = 0.0943, Train Acc = 0.9630, Train F1 = 0.9452 | Val Loss = 0.1058, Val Acc = 0.9588, Val F1 = 0.9403\n",
      "Epoch 14: Train Loss = 0.0933, Train Acc = 0.9624, Train F1 = 0.9442 | Val Loss = 0.1022, Val Acc = 0.9600, Val F1 = 0.9422\n",
      "Epoch 15: Train Loss = 0.0900, Train Acc = 0.9652, Train F1 = 0.9483 | Val Loss = 0.0993, Val Acc = 0.9564, Val F1 = 0.9378\n",
      "Epoch 16: Train Loss = 0.0855, Train Acc = 0.9664, Train F1 = 0.9504 | Val Loss = 0.1059, Val Acc = 0.9588, Val F1 = 0.9391\n",
      "Epoch 17: Train Loss = 0.0814, Train Acc = 0.9682, Train F1 = 0.9530 | Val Loss = 0.0946, Val Acc = 0.9612, Val F1 = 0.9436\n",
      "Epoch 18: Train Loss = 0.0800, Train Acc = 0.9691, Train F1 = 0.9544 | Val Loss = 0.1015, Val Acc = 0.9624, Val F1 = 0.9448\n",
      "Epoch 19: Train Loss = 0.0761, Train Acc = 0.9697, Train F1 = 0.9553 | Val Loss = 0.0903, Val Acc = 0.9612, Val F1 = 0.9436\n",
      "Epoch 20: Train Loss = 0.0758, Train Acc = 0.9718, Train F1 = 0.9584 | Val Loss = 0.0867, Val Acc = 0.9636, Val F1 = 0.9473\n",
      "Epoch 21: Train Loss = 0.0739, Train Acc = 0.9712, Train F1 = 0.9576 | Val Loss = 0.0861, Val Acc = 0.9576, Val F1 = 0.9401\n",
      "Epoch 22: Train Loss = 0.0692, Train Acc = 0.9721, Train F1 = 0.9590 | Val Loss = 0.0875, Val Acc = 0.9648, Val F1 = 0.9484\n",
      "Epoch 23: Train Loss = 0.0685, Train Acc = 0.9730, Train F1 = 0.9605 | Val Loss = 0.0862, Val Acc = 0.9661, Val F1 = 0.9503\n",
      "Epoch 24: Train Loss = 0.0654, Train Acc = 0.9748, Train F1 = 0.9629 | Val Loss = 0.0857, Val Acc = 0.9624, Val F1 = 0.9473\n",
      "Epoch 25: Train Loss = 0.0656, Train Acc = 0.9745, Train F1 = 0.9625 | Val Loss = 0.0784, Val Acc = 0.9624, Val F1 = 0.9467\n",
      "Epoch 26: Train Loss = 0.0671, Train Acc = 0.9758, Train F1 = 0.9643 | Val Loss = 0.0805, Val Acc = 0.9673, Val F1 = 0.9521\n",
      "Epoch 27: Train Loss = 0.0628, Train Acc = 0.9761, Train F1 = 0.9648 | Val Loss = 0.0836, Val Acc = 0.9673, Val F1 = 0.9517\n",
      "Epoch 28: Train Loss = 0.0587, Train Acc = 0.9794, Train F1 = 0.9696 | Val Loss = 0.0723, Val Acc = 0.9673, Val F1 = 0.9527\n",
      "Epoch 29: Train Loss = 0.0581, Train Acc = 0.9782, Train F1 = 0.9678 | Val Loss = 0.0732, Val Acc = 0.9673, Val F1 = 0.9538\n",
      "Epoch 30: Train Loss = 0.0583, Train Acc = 0.9785, Train F1 = 0.9684 | Val Loss = 0.0687, Val Acc = 0.9661, Val F1 = 0.9512\n",
      "Epoch 31: Train Loss = 0.0550, Train Acc = 0.9806, Train F1 = 0.9716 | Val Loss = 0.0813, Val Acc = 0.9697, Val F1 = 0.9551\n",
      "Epoch 32: Train Loss = 0.0561, Train Acc = 0.9788, Train F1 = 0.9687 | Val Loss = 0.0688, Val Acc = 0.9697, Val F1 = 0.9559\n",
      "Epoch 33: Train Loss = 0.0568, Train Acc = 0.9791, Train F1 = 0.9694 | Val Loss = 0.0824, Val Acc = 0.9709, Val F1 = 0.9568\n",
      "Epoch 34: Train Loss = 0.0553, Train Acc = 0.9773, Train F1 = 0.9666 | Val Loss = 0.0651, Val Acc = 0.9673, Val F1 = 0.9527\n",
      "Epoch 35: Train Loss = 0.0606, Train Acc = 0.9767, Train F1 = 0.9658 | Val Loss = 0.0634, Val Acc = 0.9685, Val F1 = 0.9545\n",
      "Epoch 36: Train Loss = 0.0500, Train Acc = 0.9821, Train F1 = 0.9737 | Val Loss = 0.0617, Val Acc = 0.9697, Val F1 = 0.9565\n",
      "Epoch 37: Train Loss = 0.0484, Train Acc = 0.9824, Train F1 = 0.9742 | Val Loss = 0.0637, Val Acc = 0.9721, Val F1 = 0.9592\n",
      "Epoch 38: Train Loss = 0.0472, Train Acc = 0.9836, Train F1 = 0.9760 | Val Loss = 0.0621, Val Acc = 0.9721, Val F1 = 0.9603\n",
      "Epoch 39: Train Loss = 0.0488, Train Acc = 0.9818, Train F1 = 0.9733 | Val Loss = 0.0587, Val Acc = 0.9709, Val F1 = 0.9584\n",
      "Epoch 40: Train Loss = 0.0448, Train Acc = 0.9845, Train F1 = 0.9773 | Val Loss = 0.0604, Val Acc = 0.9721, Val F1 = 0.9595\n",
      "Epoch 41: Train Loss = 0.0475, Train Acc = 0.9827, Train F1 = 0.9746 | Val Loss = 0.0618, Val Acc = 0.9733, Val F1 = 0.9625\n",
      "Epoch 42: Train Loss = 0.0438, Train Acc = 0.9842, Train F1 = 0.9769 | Val Loss = 0.0601, Val Acc = 0.9758, Val F1 = 0.9646\n",
      "Epoch 43: Train Loss = 0.0433, Train Acc = 0.9848, Train F1 = 0.9778 | Val Loss = 0.0595, Val Acc = 0.9733, Val F1 = 0.9625\n",
      "Epoch 44: Train Loss = 0.0437, Train Acc = 0.9842, Train F1 = 0.9770 | Val Loss = 0.0582, Val Acc = 0.9745, Val F1 = 0.9629\n",
      "Epoch 45: Train Loss = 0.0462, Train Acc = 0.9839, Train F1 = 0.9765 | Val Loss = 0.0535, Val Acc = 0.9758, Val F1 = 0.9652\n",
      "Epoch 46: Train Loss = 0.0405, Train Acc = 0.9848, Train F1 = 0.9777 | Val Loss = 0.0548, Val Acc = 0.9745, Val F1 = 0.9640\n",
      "Epoch 47: Train Loss = 0.0436, Train Acc = 0.9873, Train F1 = 0.9814 | Val Loss = 0.0550, Val Acc = 0.9758, Val F1 = 0.9648\n",
      "Epoch 48: Train Loss = 0.0386, Train Acc = 0.9882, Train F1 = 0.9827 | Val Loss = 0.0525, Val Acc = 0.9758, Val F1 = 0.9652\n"
     ]
    }
   ],
   "source": [
    "accuracy_ls, f1_score_ls, recall_score_ls, precision_score_ls = loso_training(\"/home/van/NamQuang/Dataset/WESAD_LOSO\", 700,1,1,'test_loso.csv',  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cce3c0-c850-497e-a18f-76e0610f67a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2219b-ae89-4086-b5dc-8070c25344c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
